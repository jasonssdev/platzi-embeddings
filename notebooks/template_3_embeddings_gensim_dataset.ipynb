{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6tfCay9SURW"
   },
   "source": [
    "# Word2Vec con Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6yrJAHISZ4r"
   },
   "source": [
    "## Instalación de librerías y carga de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSKi2-d-4dGG"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jip7NEukg99Q"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric, strip_short, stem_text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zPrihz2VEHz"
   },
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q4ptd0PoVDi3"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_corpus = load_dataset(\"large_spanish_corpus\", \"ParaCrawl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5hB0UvoHaScH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 15510649\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cHY6Nq1fXIYu"
   },
   "outputs": [],
   "source": [
    "subset = dataset_corpus['train'].select(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Su presencia en la tierra como poderes fantasma operando a través de sustitutos humanos serán expuestos para que todos los vean.',\n",
       "  'Típica imagen de Hollywood de una invasión alienígena hostil por parte de naves aéreas extraterrestres de alta tecnología.',\n",
       "  'o combinación de los anteriores?',\n",
       "  'En la visión gnóstica, la divulgación o revelación no es el momento en que las Autoridades gubernamentales anuncian la presencia de extraterrestres en la tierra, haciéndolo así \"oficial\".',\n",
       "  'Nada hecho oficial en este mundo cuenta para nada más que para otra táctica en el engaño globalista, avanzando en la agenda de las Autoridades.',\n",
       "  'Sin embargo, para que los arcontes hagan su movimiento final, deberán revelarse.',\n",
       "  'Debido a que su naturaleza es el engaño, lo harán de forma engañosa.',\n",
       "  'Incluso al mostrar su presencia, jugando su carta de triunfo sobre la especie humana, ellos van a engañar, porque eso es todo lo que ellos hacen, todo lo que pueden hacer.',\n",
       "  'La prueba de ese momento cae sobre la humanidad: de ver cómo las Autoridades están estafando al mundo con su magia engañosa del control de la mente, las tan cacareada operaciones psicológicas globales.',\n",
       "  'Sophia tiene la intención de que la humanidad tenga la oportunidad de llevar a cabo el experimento divino que ella pensó originalmente para ella, libre de la desviación arcóntica - es decir, libre de la falsa coerción de las Autoridades que afirman que su agenda de dominación asegura la seguridad social y libertad del caos moral .']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lavado de cerebro a través de los medios de comunicación, y amenaza de fuerza a través de los militares.',\n",
       " 'Sin un constante aluvión de doble cañón, requiriendo la complicidad de los seres humanos para reprimir y engañar a sus semejantes, su tan cacareada magia rápidamente se desvanecería y se disiparía.',\n",
       " 'En realidad, el Nuevo OM sólo se puede mantener la ilusión de supremacía mágica, siempre y cuando reprima y desvíe el potencial humano, donde mora la verdadera magia: es decir, en la capacidad innata de nuestra especie de magia interactiva con los poderes de animación de la diosa planetaria.',\n",
       " 'A menos que el Nuevo OM pueda todo el tiempo suprimir la capacidad de una manera brutal, natural y espontáneamente se afirmará a sí misma.',\n",
       " 'Cuando lo haga, la verdadera magia del Anthropos, el \"niño luminoso,\" entrará de inmediato en acción.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import EXTERNAL_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases: ['Hola, ¿cómo estás?', 'Esto es una prueba de tokenización.']\n",
      "Tokens: ['Hola', ',', '¿cómo', 'estás', '?', 'Esto', 'es', 'una', 'prueba', 'de', 'tokenización', '.']\n",
      "Tokens sin stopwords: ['Hola', ',', '¿cómo', '?', 'prueba', 'tokenización', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jssdev/Dev/Learning/Platzi/platzi-\n",
      "[nltk_data]     embeddings/data/external...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jssdev/Dev/Learning/Platzi/platzi-\n",
      "[nltk_data]     embeddings/data/external...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.data import path as nltk_path\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ruta personalizada\n",
    "custom_path = EXTERNAL_DIR\n",
    "nltk_path.clear()\n",
    "nltk_path.append(custom_path)\n",
    "\n",
    "# Descargar recursos\n",
    "nltk.download('punkt', download_dir=custom_path)\n",
    "nltk.download('stopwords', download_dir=custom_path)\n",
    "\n",
    "# Cargar tokenizer de oraciones desde el .pickle (no falla)\n",
    "spanish_pickle_path = os.path.join(custom_path, \"tokenizers\", \"punkt\", \"spanish.pickle\")\n",
    "with open(spanish_pickle_path, \"rb\") as f:\n",
    "    sentence_tokenizer = pickle.load(f)\n",
    "\n",
    "# Reemplazo de word_tokenize sin usar sent_tokenize()\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"Hola, ¿cómo estás? Esto es una prueba de tokenización.\"\n",
    "\n",
    "# Procesamiento\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "tokens = word_tokenizer.tokenize(text)  # SIN usar word_tokenize()\n",
    "filtered = [t for t in tokens if t.lower() not in stopwords.words(\"spanish\")]\n",
    "\n",
    "# Resultado\n",
    "print(\"Frases:\", sentences)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Tokens sin stopwords:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbtKlKd1UmF_"
   },
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cpOhY7jyRATr"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric, strip_short\n",
    "from nltk.data import path as nltk_path\n",
    "import os\n",
    "\n",
    "def clean_text(sentence_batch):\n",
    "    # Ajusta path a recursos NLTK\n",
    "    custom_path = EXTERNAL_DIR\n",
    "    nltk_path.clear()\n",
    "    nltk_path.append(custom_path)\n",
    "\n",
    "    # Tokenizador sin dependencia de sent_tokenize\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "    cleaned_text_list = []\n",
    "\n",
    "    for text in sentence_batch['text']:\n",
    "        # Minúsculas\n",
    "        text = text.lower()\n",
    "\n",
    "        # Eliminar URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "        # Eliminar menciones y hashtags\n",
    "        text = re.sub(r'\\@\\w+|\\#\\w+', '', text)\n",
    "\n",
    "        # Eliminar puntuación, números, palabras cortas\n",
    "        text = strip_punctuation(text)\n",
    "        text = strip_numeric(text)\n",
    "        text = strip_short(text, minsize=2)\n",
    "\n",
    "        # Tokenizar y filtrar stopwords\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        filtered = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        cleaned_text_list.append(filtered)\n",
    "\n",
    "    return {'text': cleaned_text_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915c2dcf71914fb1a3305e6561b771d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences_corpus = subset.map(clean_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xC_1SRrnYJWv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lavado',\n",
       "  'cerebro',\n",
       "  'través',\n",
       "  'medios',\n",
       "  'comunicación',\n",
       "  'amenaza',\n",
       "  'fuerza',\n",
       "  'través',\n",
       "  'militares'],\n",
       " ['constante',\n",
       "  'aluvión',\n",
       "  'doble',\n",
       "  'cañón',\n",
       "  'requiriendo',\n",
       "  'complicidad',\n",
       "  'seres',\n",
       "  'humanos',\n",
       "  'reprimir',\n",
       "  'engañar',\n",
       "  'semejantes',\n",
       "  'tan',\n",
       "  'cacareada',\n",
       "  'magia',\n",
       "  'rápidamente',\n",
       "  'desvanecería',\n",
       "  'disiparía'],\n",
       " ['realidad',\n",
       "  'nuevo',\n",
       "  'om',\n",
       "  'sólo',\n",
       "  'puede',\n",
       "  'mantener',\n",
       "  'ilusión',\n",
       "  'supremacía',\n",
       "  'mágica',\n",
       "  'siempre',\n",
       "  'reprima',\n",
       "  'desvíe',\n",
       "  'potencial',\n",
       "  'humano',\n",
       "  'mora',\n",
       "  'verdadera',\n",
       "  'magia',\n",
       "  'decir',\n",
       "  'capacidad',\n",
       "  'innata',\n",
       "  'especie',\n",
       "  'magia',\n",
       "  'interactiva',\n",
       "  'poderes',\n",
       "  'animación',\n",
       "  'diosa',\n",
       "  'planetaria']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_corpus['text'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J-HBL-FVxKc"
   },
   "source": [
    "## Carga y uso de modelo de embeddings Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7E121yKbljm"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec()\n",
    "\n",
    "# Podemos guardar el modelo para uso futuro\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-Qho2ckg0xd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1ssleSObsUc"
   },
   "outputs": [],
   "source": [
    "##comida, ser, reina, television\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytaIZOJHiUrj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y9DpAyLWCVN"
   },
   "source": [
    "## Almacenamiento de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYeA70cVnQ_J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tiIoHVLn85G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "emb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
